networks:
  appnet:
    driver: bridge

volumes:
  ollama-data:


services:
  fastapi:
    build:
      context: ./server/FastAPI
    ports:
      - "8000:8000"
    environment:
      OLLAMA_HOST: http://rust-api:8080
    depends_on:
      - rust-api
    networks:
      - appnet

  rust-api:
    build:
      context: ./server/RUST_TAURI/ollama_server
    ports:
      - "8080:8080"
    environment:
      OLLAMA_HOST: http://llama-cpp:11434
    depends_on:
      - llama-cpp
    networks:
      - appnet
    volumes:
      - ./server/RUST_TAURI/ollama_server/topics.txt:/app/topics.txt

  llama-cpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    ports:
      - "11434:11434"
    networks:
      - appnet
    volumes:
      - ./models:/models
      - ./start-llama.sh:/start-llama.sh
    entrypoint: [ "/bin/sh", "/start-llama.sh" ]
